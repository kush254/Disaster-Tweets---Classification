{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['keyword'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3271, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create different set for True and False train sets\n",
    "train_true = train[train['target']==1]\n",
    "train_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4342, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_false = train[train['target']==0]\n",
    "train_false.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wreckage       39\n",
       "outbreak       39\n",
       "derailment     39\n",
       "typhoon        37\n",
       "oil%20spill    37\n",
       "               ..\n",
       "epicentre       1\n",
       "ruin            1\n",
       "electrocute     1\n",
       "blazing         1\n",
       "body%20bags     1\n",
       "Name: keyword, Length: 220, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keywords used in true cases\n",
    "keyword_true = train_true['keyword'].value_counts()\n",
    "keyword_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body%20bags          40\n",
       "armageddon           37\n",
       "harm                 37\n",
       "deluge               36\n",
       "ruin                 36\n",
       "                     ..\n",
       "suicide%20bombing     1\n",
       "oil%20spill           1\n",
       "suicide%20bomber      1\n",
       "typhoon               1\n",
       "outbreak              1\n",
       "Name: keyword, Length: 218, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keywords used in false cases\n",
    "keyword_false = train_false['keyword'].value_counts()\n",
    "keyword_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_true['keyword'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body%20bags          40\n",
       "armageddon           37\n",
       "harm                 37\n",
       "deluge               36\n",
       "ruin                 36\n",
       "                     ..\n",
       "suicide%20bombing     1\n",
       "oil%20spill           1\n",
       "suicide%20bomber      1\n",
       "typhoon               1\n",
       "outbreak              1\n",
       "Name: keyword, Length: 218, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_false['keyword'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_false['keyword'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USA                     67\n",
       "United States           27\n",
       "Nigeria                 22\n",
       "India                   20\n",
       "Mumbai                  19\n",
       "                        ..\n",
       "ss                       1\n",
       "Nashville, Tennessee     1\n",
       "Halton, Ontario          1\n",
       "PDX                      1\n",
       "San Mateo County, CA     1\n",
       "Name: location, Length: 1513, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_true['location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "New York                         55\n",
       "USA                              37\n",
       "London                           29\n",
       "United States                    23\n",
       "Los Angeles, CA                  18\n",
       "                                 ..\n",
       "Basking Ridge, NJ                 1\n",
       "East London.                      1\n",
       "Dayton, OH                        1\n",
       "Extraterrestrial Highway          1\n",
       "?? Made in the Philippines ??     1\n",
       "Name: location, Length: 2142, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_false['location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1075"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_true['location'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_false['location'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0     1.0  \n",
       "1     1.0  \n",
       "2     1.0  \n",
       "3     1.0  \n",
       "4     1.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.concat([train,test],axis=0)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2d709238588>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df_all.isnull(),yticklabels=False,cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       87\n",
       "location    3638\n",
       "text           0\n",
       "target      3263\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "New York                      38\n",
       "USA                           37\n",
       "Worldwide                     16\n",
       "United States                 15\n",
       "London                        13\n",
       "                              ..\n",
       "Citizen of the World           1\n",
       "centre of attention            1\n",
       "Niger State                    1\n",
       "Franklinton - BR - Houston     1\n",
       "West Coast, Canada             1\n",
       "Name: location, Length: 1602, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1105"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['location'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     69\n",
       "1     38\n",
       "2    133\n",
       "3     65\n",
       "4     88\n",
       "Name: text_len, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new feature length of text\n",
    "df_all['text_len'] = df_all['text'].apply(lambda x : len(x))\n",
    "df_all['text_len'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_letters_count(sentence):\n",
    "    sum1 = 0\n",
    "    for c in sentence:\n",
    "        if c.isupper():\n",
    "            sum1 = sum1 + 1\n",
    "    return sum1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10\n",
       "1     5\n",
       "2     2\n",
       "3     1\n",
       "4     3\n",
       "Name: capital_letters, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of capital letters\n",
    "df_all['capital_letters'] = df_all['text'].apply(lambda x : capital_letters_count(x))\n",
    "df_all['capital_letters'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      868\n",
       "2      770\n",
       "3      700\n",
       "4      700\n",
       "6      675\n",
       "      ... \n",
       "98       1\n",
       "78       1\n",
       "118      1\n",
       "87       1\n",
       "97       1\n",
       "Name: capital_letters, Length: 96, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['capital_letters'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_punctuation(sentence):\n",
    "    sum2 = 0 \n",
    "    for c in sentence:\n",
    "        if c in string.punctuation:\n",
    "            sum2 = sum2 + 1\n",
    "    return sum2\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing the function\n",
    "sen = 'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'\n",
    "sen2 = 'What a goooooooaaaaaal!!!!!!'\n",
    "s = count_punctuation(sen2)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    3\n",
       "3    2\n",
       "4    2\n",
       "Name: punctuations, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['punctuations'] = df_all.text.apply(lambda x : count_punctuation(x))\n",
    "df_all['punctuations'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['location'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    nan Our Deeds are the Reason of this #earthqua...\n",
       "1           nan Forest fire near La Ronge Sask. Canada\n",
       "2    nan All residents asked to 'shelter in place' ...\n",
       "3    nan 13,000 people receive #wildfires evacuatio...\n",
       "4    nan Just got sent this photo from Ruby #Alaska...\n",
       "Name: location_text, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['location_text'] = df_all['location'].astype(str) + ' ' + df_all['text'].astype(str)\n",
    "df_all['location_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to check if a HTTP link exists in text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_len</th>\n",
       "      <th>capital_letters</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>location_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>nan Our Deeds are the Reason of this #earthqua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>nan Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>133</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>nan All residents asked to 'shelter in place' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>nan 13,000 people receive #wildfires evacuatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>nan Just got sent this photo from Ruby #Alaska...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  text_len  capital_letters  punctuations  \\\n",
       "0     1.0        69               10             1   \n",
       "1     1.0        38                5             1   \n",
       "2     1.0       133                2             3   \n",
       "3     1.0        65                1             2   \n",
       "4     1.0        88                3             2   \n",
       "\n",
       "                                       location_text  \n",
       "0  nan Our Deeds are the Reason of this #earthqua...  \n",
       "1         nan Forest fire near La Ronge Sask. Canada  \n",
       "2  nan All residents asked to 'shelter in place' ...  \n",
       "3  nan 13,000 people receive #wildfires evacuatio...  \n",
       "4  nan Just got sent this photo from Ruby #Alaska...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'nan']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chachedWords = stopwords.words('english')\n",
    "chachedWords.append('nan')\n",
    "chachedWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing punctuation and stop words function\n",
    "def clean_text(sentence):\n",
    "    ps = nltk.PorterStemmer()\n",
    "    #nltk.download(\"stopwords\")\n",
    "    chachedWords = stopwords.words('english')\n",
    "    chachedWords.append('nan')\n",
    "    temp = \"\"\n",
    "    return_sentence = \"\"\n",
    "    sentence = sentence.lower()\n",
    "    for c in sentence:\n",
    "        if c not in string.punctuation:\n",
    "            temp = temp+c\n",
    "    temp_arr = temp.split()\n",
    "    for word in temp_arr:\n",
    "        if word not in chachedWords:\n",
    "            word = ps.stem(word)\n",
    "            return_sentence = return_sentence + word + \" \"\n",
    "    return_sentence = return_sentence.strip()\n",
    "    return return_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\i318517\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0            deed reason earthquak may allah forgiv us\n",
       "1                 forest fire near la rong sask canada\n",
       "2    resid ask shelter place notifi offic evacu she...\n",
       "3    13000 peopl receiv wildfir evacu order california\n",
       "4    got sent photo rubi alaska smoke wildfir pour ...\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nan = ['nan']\n",
    "df_all['clean_text'] = df_all.location_text.apply(lambda x: clean_text(x))\n",
    "df_all['clean_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only remove puntuation\n",
    "def remove_punctuation(sentence):\n",
    "    ps = nltk.PorterStemmer()\n",
    "    null_value = ['nan']\n",
    "    temp = \"\"\n",
    "    return_sentence = \"\"\n",
    "    sentence = sentence.lower()\n",
    "    for c in sentence:\n",
    "        if c not in string.punctuation:\n",
    "            temp = temp+c\n",
    "    temp_arr = temp.split()\n",
    "    for word in temp_arr:\n",
    "        if word not in null_value:\n",
    "            return_sentence = return_sentence + word + \" \"\n",
    "    return_sentence = return_sentence.strip()\n",
    "    return return_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    our deeds are the reason of this earthquake ma...\n",
       "1                forest fire near la ronge sask canada\n",
       "2    all residents asked to shelter in place are be...\n",
       "3    13000 people receive wildfires evacuation orde...\n",
       "4    just got sent this photo from ruby alaska as s...\n",
       "Name: remove_punctuations, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['remove_punctuations'] = df_all.text.apply(lambda x : remove_punctuation(x))\n",
    "df_all['remove_punctuations'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>text_len</th>\n",
       "      <th>capital_letters</th>\n",
       "      <th>punctuations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10876.000000</td>\n",
       "      <td>7613.00000</td>\n",
       "      <td>10876.000000</td>\n",
       "      <td>10876.000000</td>\n",
       "      <td>10876.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5437.500000</td>\n",
       "      <td>0.42966</td>\n",
       "      <td>101.358680</td>\n",
       "      <td>10.041743</td>\n",
       "      <td>6.872839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3139.775098</td>\n",
       "      <td>0.49506</td>\n",
       "      <td>33.840687</td>\n",
       "      <td>10.746684</td>\n",
       "      <td>4.572513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2718.750000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5437.500000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8156.250000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10875.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id      target      text_len  capital_letters  punctuations\n",
       "count  10876.000000  7613.00000  10876.000000     10876.000000  10876.000000\n",
       "mean    5437.500000     0.42966    101.358680        10.041743      6.872839\n",
       "std     3139.775098     0.49506     33.840687        10.746684      4.572513\n",
       "min        0.000000     0.00000      5.000000         0.000000      0.000000\n",
       "25%     2718.750000     0.00000     78.000000         3.000000      3.000000\n",
       "50%     5437.500000     0.00000    108.000000         8.000000      6.000000\n",
       "75%     8156.250000     1.00000    134.000000        13.000000     10.000000\n",
       "max    10875.000000     1.00000    157.000000       118.000000     61.000000"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>target</th>\n",
       "      <th>text_len</th>\n",
       "      <th>capital_letters</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>remove_punctuations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>133</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order california</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword  target  text_len  capital_letters  punctuations  \\\n",
       "0   1     NaN     1.0        69               10             1   \n",
       "1   4     NaN     1.0        38                5             1   \n",
       "2   5     NaN     1.0       133                2             3   \n",
       "3   6     NaN     1.0        65                1             2   \n",
       "4   7     NaN     1.0        88                3             2   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0          deed reason earthquak may allah forgiv us   \n",
       "1               forest fire near la rong sask canada   \n",
       "2  resid ask shelter place notifi offic evacu she...   \n",
       "3  13000 peopl receiv wildfir evacu order california   \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...   \n",
       "\n",
       "                                 remove_punctuations  \n",
       "0  our deeds are the reason of this earthquake ma...  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  all residents asked to shelter in place are be...  \n",
       "3  13000 people receive wildfires evacuation orde...  \n",
       "4  just got sent this photo from ruby alaska as s...  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_feature = df_all.drop(['location','text','location_text'],axis=1)\n",
    "df_all_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 221)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "suicide%20bomb           50\n",
       "catastrophic             50\n",
       "harm                     50\n",
       "rubble                   50\n",
       "wounds                   50\n",
       "sirens                   50\n",
       "hazard                   50\n",
       "ambulance                50\n",
       "rainstorm                50\n",
       "debris                   50\n",
       "injury                   50\n",
       "casualty                 50\n",
       "burning%20buildings      50\n",
       "crash                    50\n",
       "buildings%20on%20fire    50\n",
       "devastated               50\n",
       "buildings%20burning      50\n",
       "wrecked                  50\n",
       "survived                 50\n",
       "disaster                 50\n",
       "electrocuted             50\n",
       "cyclone                  50\n",
       "blood                    50\n",
       "tragedy                  50\n",
       "attack                   50\n",
       "mayhem                   50\n",
       "arsonist                 50\n",
       "burned                   50\n",
       "heat%20wave              50\n",
       "loud%20bang              50\n",
       "explode                  50\n",
       "aftershock               50\n",
       "detonate                 50\n",
       "forest%20fires           50\n",
       "airplane%20accident      50\n",
       "deaths                   50\n",
       "blazing                  50\n",
       "trapped                  50\n",
       "arson                    50\n",
       "quarantined              50\n",
       "hostage                  50\n",
       "destroy                  50\n",
       "bioterror                50\n",
       "displaced                50\n",
       "annihilated              50\n",
       "emergency                50\n",
       "panicking                50\n",
       "fatalities               50\n",
       "detonation               50\n",
       "flood                    50\n",
       "drowned                  50\n",
       "catastrophe              50\n",
       "violent%20storm          50\n",
       "terrorism                50\n",
       "avalanche                50\n",
       "dust%20storm             50\n",
       "wreckage                 50\n",
       "suicide%20bomber         50\n",
       "thunderstorm             50\n",
       "suicide%20bombing        50\n",
       "demolished               50\n",
       "structural%20failure     50\n",
       "weapon                   50\n",
       "inundated                50\n",
       "hazardous                50\n",
       "mass%20murder            50\n",
       "devastation              50\n",
       "snowstorm                50\n",
       "floods                   50\n",
       "military                 50\n",
       "sinking                  50\n",
       "crushed                  50\n",
       "engulfed                 50\n",
       "fire%20truck             50\n",
       "oil%20spill              50\n",
       "panic                    50\n",
       "hailstorm                50\n",
       "destruction              50\n",
       "hijack                   50\n",
       "death                    50\n",
       "danger                   50\n",
       "fatality                 50\n",
       "meltdown                 50\n",
       "emergency%20services     50\n",
       "smoke                    50\n",
       "collide                  50\n",
       "deluge                   50\n",
       "flattened                50\n",
       "rioting                  50\n",
       "razed                    50\n",
       "bombed                   50\n",
       "drown                    50\n",
       "collided                 50\n",
       "bomb                     50\n",
       "tornado                  50\n",
       "eyewitness               50\n",
       "bridge%20collapse        50\n",
       "destroyed                50\n",
       "desolation               50\n",
       "survivors                50\n",
       "demolition               50\n",
       "typhoon                  50\n",
       "electrocute              50\n",
       "drought                  50\n",
       "tsunami                  50\n",
       "ruin                     50\n",
       "crush                    50\n",
       "rescued                  50\n",
       "evacuated                50\n",
       "screams                  50\n",
       "trauma                   50\n",
       "drowning                 50\n",
       "seismic                  50\n",
       "casualties               50\n",
       "screamed                 50\n",
       "terrorist                50\n",
       "siren                    50\n",
       "accident                 50\n",
       "police                   50\n",
       "fire                     50\n",
       "natural%20disaster       50\n",
       "injured                  50\n",
       "injuries                 50\n",
       "flames                   50\n",
       "ablaze                   50\n",
       "hellfire                 50\n",
       "rescuers                 50\n",
       "earthquake               50\n",
       "massacre                 50\n",
       "stretcher                50\n",
       "flooding                 50\n",
       "explosion                50\n",
       "sunk                     50\n",
       "bioterrorism             50\n",
       "cliff%20fall             50\n",
       "riot                     50\n",
       "sinkhole                 50\n",
       "burning                  50\n",
       "lightning                50\n",
       "blew%20up                50\n",
       "derailment               50\n",
       "blizzard                 50\n",
       "evacuation               50\n",
       "curfew                   50\n",
       "landslide                50\n",
       "hurricane                50\n",
       "bloody                   50\n",
       "armageddon               50\n",
       "wildfire                 50\n",
       "obliterated              50\n",
       "upheaval                 50\n",
       "storm                    50\n",
       "mudslide                 50\n",
       "army                     50\n",
       "derailed                 50\n",
       "obliterate               50\n",
       "collapsed                50\n",
       "outbreak                 50\n",
       "damage                   50\n",
       "nuclear%20disaster       50\n",
       "dead                     50\n",
       "quarantine               50\n",
       "blown%20up               50\n",
       "traumatised              50\n",
       "lava                     50\n",
       "attacked                 50\n",
       "hail                     50\n",
       "sandstorm                50\n",
       "body%20bagging           50\n",
       "crashed                  50\n",
       "twister                  50\n",
       "wreck                    50\n",
       "blaze                    50\n",
       "blight                   50\n",
       "fatal                    50\n",
       "hostages                 50\n",
       "hijacking                50\n",
       "famine                   50\n",
       "fear                     50\n",
       "screaming                50\n",
       "nuclear%20reactor        50\n",
       "refugees                 50\n",
       "hijacker                 50\n",
       "survive                  50\n",
       "apocalypse               50\n",
       "weapons                  50\n",
       "thunder                  50\n",
       "first%20responders       50\n",
       "windstorm                50\n",
       "body%20bags              50\n",
       "collapse                 50\n",
       "derail                   50\n",
       "bleeding                 50\n",
       "wounded                  50\n",
       "deluged                  50\n",
       "emergency%20plan         50\n",
       "annihilation             50\n",
       "body%20bag               50\n",
       "wild%20fires             50\n",
       "trouble                  50\n",
       "exploded                 50\n",
       "obliteration             50\n",
       "evacuate                 50\n",
       "whirlwind                50\n",
       "pandemonium              50\n",
       "demolish                 50\n",
       "collision                49\n",
       "mass%20murderer          46\n",
       "chemical%20emergency     45\n",
       "desolate                 45\n",
       "bombing                  42\n",
       "volcano                  42\n",
       "bush%20fires             38\n",
       "war%20zone               35\n",
       "battle                   33\n",
       "rescue                   33\n",
       "forest%20fire            24\n",
       "threat                   16\n",
       "inundation               14\n",
       "radiation%20emergency    14\n",
       "epicentre                13\n",
       "Name: keyword, dtype: int64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_feature['keyword'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ablaze', 'accident', 'aftershock', 'airplane%20accident',\n",
       "       'ambulance', 'annihilated', 'annihilation', 'apocalypse',\n",
       "       'armageddon', 'army', 'arson', 'arsonist', 'attack', 'attacked',\n",
       "       'avalanche', 'battle', 'bioterror', 'bioterrorism', 'blaze',\n",
       "       'blazing', 'bleeding', 'blew%20up', 'blight', 'blizzard', 'blood',\n",
       "       'bloody', 'blown%20up', 'body%20bag', 'body%20bagging',\n",
       "       'body%20bags', 'bomb', 'bombed', 'bombing', 'bridge%20collapse',\n",
       "       'buildings%20burning', 'buildings%20on%20fire', 'burned',\n",
       "       'burning', 'burning%20buildings', 'bush%20fires', 'casualties',\n",
       "       'casualty', 'catastrophe', 'catastrophic', 'chemical%20emergency',\n",
       "       'cliff%20fall', 'collapse', 'collapsed', 'collide', 'collided',\n",
       "       'collision', 'crash', 'crashed', 'crush', 'crushed', 'curfew',\n",
       "       'cyclone', 'damage', 'danger', 'dead', 'death', 'deaths', 'debris',\n",
       "       'deluge', 'deluged', 'demolish', 'demolished', 'demolition',\n",
       "       'derail', 'derailed', 'derailment', 'desolate', 'desolation',\n",
       "       'destroy', 'destroyed', 'destruction', 'detonate', 'detonation',\n",
       "       'devastated', 'devastation', 'disaster', 'displaced', 'drought',\n",
       "       'drown', 'drowned', 'drowning', 'dust%20storm', 'earthquake',\n",
       "       'electrocute', 'electrocuted', 'emergency', 'emergency%20plan',\n",
       "       'emergency%20services', 'engulfed', 'epicentre', 'evacuate',\n",
       "       'evacuated', 'evacuation', 'explode', 'exploded', 'explosion',\n",
       "       'eyewitness', 'famine', 'fatal', 'fatalities', 'fatality', 'fear',\n",
       "       'fire', 'fire%20truck', 'first%20responders', 'flames',\n",
       "       'flattened', 'flood', 'flooding', 'floods', 'forest%20fire',\n",
       "       'forest%20fires', 'hail', 'hailstorm', 'harm', 'hazard',\n",
       "       'hazardous', 'heat%20wave', 'hellfire', 'hijack', 'hijacker',\n",
       "       'hijacking', 'hostage', 'hostages', 'hurricane', 'injured',\n",
       "       'injuries', 'injury', 'inundated', 'inundation', 'landslide',\n",
       "       'lava', 'lightning', 'loud%20bang', 'mass%20murder',\n",
       "       'mass%20murderer', 'massacre', 'mayhem', 'meltdown', 'military',\n",
       "       'mudslide', 'natural%20disaster', 'nuclear%20disaster',\n",
       "       'nuclear%20reactor', 'obliterate', 'obliterated', 'obliteration',\n",
       "       'oil%20spill', 'outbreak', 'pandemonium', 'panic', 'panicking',\n",
       "       'police', 'quarantine', 'quarantined', 'radiation%20emergency',\n",
       "       'rainstorm', 'razed', 'refugees', 'rescue', 'rescued', 'rescuers',\n",
       "       'riot', 'rioting', 'rubble', 'ruin', 'sandstorm', 'screamed',\n",
       "       'screaming', 'screams', 'seismic', 'sinkhole', 'sinking', 'siren',\n",
       "       'sirens', 'smoke', 'snowstorm', 'storm', 'stretcher',\n",
       "       'structural%20failure', 'suicide%20bomb', 'suicide%20bomber',\n",
       "       'suicide%20bombing', 'sunk', 'survive', 'survived', 'survivors',\n",
       "       'terrorism', 'terrorist', 'threat', 'thunder', 'thunderstorm',\n",
       "       'tornado', 'tragedy', 'trapped', 'trauma', 'traumatised',\n",
       "       'trouble', 'tsunami', 'twister', 'typhoon', 'upheaval',\n",
       "       'violent%20storm', 'volcano', 'war%20zone', 'weapon', 'weapons',\n",
       "       'whirlwind', 'wild%20fires', 'wildfire', 'windstorm', 'wounded',\n",
       "       'wounds', 'wreck', 'wreckage', 'wrecked'], dtype=object)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = df_all_feature['keyword'].unique()\n",
    "keywords = np.delete(keywords,0)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_feature['keyword'] = df_all['keyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = []\n",
    "def fill_missing_keyword_values(sentence):\n",
    "    temp = sentence.split()\n",
    "    return_word = np.nan\n",
    "    for word in temp:\n",
    "        if word in keywords:\n",
    "            return_word = word\n",
    "            break\n",
    "    all_keywords.append(return_word)\n",
    "    print(return_word)\n",
    "    return return_word\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_feature['keyword'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['earthquake',\n",
       " 'fire',\n",
       " 'evacuation',\n",
       " 'evacuation',\n",
       " 'smoke',\n",
       " 'fire',\n",
       " 'flood',\n",
       " 'fire',\n",
       " 'emergency',\n",
       " 'tornado',\n",
       " nan,\n",
       " 'flooding',\n",
       " 'flooding',\n",
       " 'flood',\n",
       " 'damage',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'ablaze',\n",
       " nan,\n",
       " 'ablaze',\n",
       " 'ablaze',\n",
       " 'ablaze',\n",
       " 'ablaze',\n",
       " 'ablaze',\n",
       " 'ablaze',\n",
       " 'ablaze',\n",
       " nan,\n",
       " 'ablaze',\n",
       " 'ablaze',\n",
       " 'ablaze',\n",
       " 'ablaze',\n",
       " nan,\n",
       " 'burned',\n",
       " 'ablaze',\n",
       " nan,\n",
       " nan,\n",
       " 'ablaze',\n",
       " 'ablaze',\n",
       " 'police',\n",
       " 'police',\n",
       " nan,\n",
       " 'ablaze',\n",
       " 'ablaze',\n",
       " 'ablaze',\n",
       " 'ablaze',\n",
       " 'burned',\n",
       " 'ablaze',\n",
       " 'ablaze',\n",
       " 'ablaze',\n",
       " 'ablaze',\n",
       " 'ablaze',\n",
       " 'fire',\n",
       " 'ablaze',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " nan,\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " nan,\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " nan,\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'police',\n",
       " 'accident',\n",
       " nan,\n",
       " nan,\n",
       " 'aftershock',\n",
       " nan,\n",
       " 'aftershock',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'aftershock',\n",
       " 'aftershock',\n",
       " 'aftershock',\n",
       " 'aftershock',\n",
       " 'aftershock',\n",
       " 'aftershock',\n",
       " 'aftershock',\n",
       " 'aftershock',\n",
       " 'aftershock',\n",
       " 'aftershock',\n",
       " nan,\n",
       " 'aftershock',\n",
       " 'screaming',\n",
       " 'aftershock',\n",
       " 'aftershock',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'aftershock',\n",
       " nan,\n",
       " 'aftershock',\n",
       " 'aftershock',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'debris',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'debris',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'debris',\n",
       " 'debris',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'debris',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'dead',\n",
       " 'crash',\n",
       " 'accident',\n",
       " 'debris',\n",
       " 'debris',\n",
       " 'accident',\n",
       " 'crashed',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'debris',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'accident',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'crash',\n",
       " 'ambulance',\n",
       " 'emergency',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'destroy',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'police',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'ambulance',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " nan,\n",
       " 'annihilated',\n",
       " nan,\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " nan,\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilated',\n",
       " 'annihilation',\n",
       " 'annihilation',\n",
       " 'annihilation',\n",
       " 'annihilation',\n",
       " 'annihilation',\n",
       " 'annihilation',\n",
       " 'annihilation',\n",
       " 'annihilation',\n",
       " 'annihilation',\n",
       " nan,\n",
       " 'annihilation',\n",
       " 'annihilation',\n",
       " nan,\n",
       " 'annihilation',\n",
       " nan,\n",
       " 'annihilation',\n",
       " 'annihilation',\n",
       " 'annihilation',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'annihilation',\n",
       " 'annihilation',\n",
       " 'annihilation',\n",
       " nan,\n",
       " 'annihilation',\n",
       " 'annihilation',\n",
       " 'annihilation',\n",
       " 'annihilation',\n",
       " nan,\n",
       " 'survive',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'storm',\n",
       " 'apocalypse',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'storm',\n",
       " 'apocalypse',\n",
       " 'apocalypse',\n",
       " 'attack',\n",
       " 'apocalypse',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'apocalypse',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'apocalypse',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " nan,\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " nan,\n",
       " 'apocalypse',\n",
       " nan,\n",
       " 'hail',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " nan,\n",
       " nan,\n",
       " 'armageddon',\n",
       " 'armageddon',\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " nan,\n",
       " 'army',\n",
       " nan,\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " nan,\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " nan,\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " 'army',\n",
       " 'arson',\n",
       " 'arson',\n",
       " 'arson',\n",
       " 'arson',\n",
       " nan,\n",
       " 'arson',\n",
       " 'arson',\n",
       " 'arson',\n",
       " 'arson',\n",
       " 'arson',\n",
       " nan,\n",
       " 'arson',\n",
       " 'arson',\n",
       " 'death',\n",
       " 'arson',\n",
       " 'police',\n",
       " nan,\n",
       " 'arson',\n",
       " 'arson',\n",
       " 'arson',\n",
       " 'arson',\n",
       " 'arson',\n",
       " 'police',\n",
       " 'arson',\n",
       " 'arson',\n",
       " 'arson',\n",
       " 'arson',\n",
       " 'arson',\n",
       " 'arson',\n",
       " 'destroyed',\n",
       " 'arson',\n",
       " 'arson',\n",
       " nan,\n",
       " 'arsonist',\n",
       " 'arsonist',\n",
       " nan,\n",
       " 'arsonist',\n",
       " 'arson',\n",
       " nan,\n",
       " nan,\n",
       " 'arsonist',\n",
       " 'arsonist',\n",
       " nan,\n",
       " 'arsonist',\n",
       " 'arsonist',\n",
       " 'burning',\n",
       " nan,\n",
       " 'arsonist',\n",
       " nan,\n",
       " 'arsonist',\n",
       " 'blaze',\n",
       " nan,\n",
       " 'arsonist',\n",
       " nan,\n",
       " 'arsonist',\n",
       " nan,\n",
       " nan,\n",
       " 'arsonist',\n",
       " 'arsonist',\n",
       " nan,\n",
       " 'smoke',\n",
       " nan,\n",
       " nan,\n",
       " 'arson',\n",
       " 'terrorism',\n",
       " 'burning',\n",
       " 'attack',\n",
       " nan,\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'bombing',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'terrorist',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'police',\n",
       " 'attack',\n",
       " 'attack',\n",
       " nan,\n",
       " nan,\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'smoke',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'attacked',\n",
       " 'injured',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'attacked',\n",
       " 'avalanche',\n",
       " 'avalanche',\n",
       " 'crash',\n",
       " 'avalanche',\n",
       " 'avalanche',\n",
       " nan,\n",
       " 'avalanche',\n",
       " 'avalanche',\n",
       " 'avalanche',\n",
       " 'avalanche',\n",
       " 'flames',\n",
       " 'avalanche',\n",
       " 'avalanche',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'avalanche',\n",
       " 'avalanche',\n",
       " 'avalanche',\n",
       " 'avalanche',\n",
       " 'avalanche',\n",
       " 'avalanche',\n",
       " nan,\n",
       " nan,\n",
       " 'avalanche',\n",
       " 'avalanche',\n",
       " 'avalanche',\n",
       " 'avalanche',\n",
       " 'avalanche',\n",
       " 'avalanche',\n",
       " 'battle',\n",
       " 'battle',\n",
       " 'battle',\n",
       " 'battle',\n",
       " 'battle',\n",
       " 'battle',\n",
       " 'battle',\n",
       " 'battle',\n",
       " 'battle',\n",
       " 'battle',\n",
       " 'battle',\n",
       " 'battle',\n",
       " 'battle',\n",
       " 'battle',\n",
       " 'battle',\n",
       " nan,\n",
       " 'battle',\n",
       " 'battle',\n",
       " 'battle',\n",
       " 'battle',\n",
       " 'battle',\n",
       " 'battle',\n",
       " 'battle',\n",
       " 'battle',\n",
       " nan,\n",
       " nan,\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " nan,\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " nan,\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " nan,\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " 'bioterror',\n",
       " nan,\n",
       " 'bioterrorism',\n",
       " 'bioterrorism',\n",
       " 'bioterrorism',\n",
       " 'bioterrorism',\n",
       " nan,\n",
       " 'bioterrorism',\n",
       " 'bioterrorism',\n",
       " 'bioterrorism',\n",
       " 'bioterrorism',\n",
       " 'bioterrorism',\n",
       " 'bioterrorism',\n",
       " 'bioterrorism',\n",
       " 'emergency',\n",
       " 'bioterrorism',\n",
       " 'bioterrorism',\n",
       " 'bioterrorism',\n",
       " 'bioterrorism',\n",
       " 'bioterrorism',\n",
       " 'hostage',\n",
       " 'bioterrorism',\n",
       " 'bioterrorism',\n",
       " 'bioterrorism',\n",
       " nan,\n",
       " nan,\n",
       " 'bioterrorism',\n",
       " nan,\n",
       " 'bioterrorism',\n",
       " 'bioterrorism',\n",
       " 'threat',\n",
       " 'bioterrorism',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'blaze',\n",
       " 'blaze',\n",
       " nan,\n",
       " nan,\n",
       " 'fire',\n",
       " nan,\n",
       " 'blaze',\n",
       " 'blaze',\n",
       " 'blaze',\n",
       " nan,\n",
       " 'wildfire',\n",
       " 'blaze',\n",
       " nan,\n",
       " nan,\n",
       " 'blaze',\n",
       " 'blaze',\n",
       " nan,\n",
       " 'blaze',\n",
       " 'blaze',\n",
       " nan,\n",
       " nan,\n",
       " 'blaze',\n",
       " 'blaze',\n",
       " nan,\n",
       " 'blaze',\n",
       " 'blaze',\n",
       " 'wildfire',\n",
       " 'blaze',\n",
       " 'blaze',\n",
       " nan,\n",
       " 'blaze',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " nan,\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " nan,\n",
       " 'weapons',\n",
       " nan,\n",
       " 'blazing',\n",
       " nan,\n",
       " 'blazing',\n",
       " nan,\n",
       " nan,\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'blazing',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " 'bleeding',\n",
       " nan,\n",
       " 'bomb',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'flames',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'blight',\n",
       " 'blight',\n",
       " 'blight',\n",
       " 'blight',\n",
       " nan,\n",
       " 'blight',\n",
       " 'blight',\n",
       " 'blight',\n",
       " 'blight',\n",
       " 'blight',\n",
       " 'blight',\n",
       " nan,\n",
       " 'blight',\n",
       " 'blight',\n",
       " 'blight',\n",
       " 'blight',\n",
       " nan,\n",
       " 'blight',\n",
       " 'blight',\n",
       " 'blight',\n",
       " 'blight',\n",
       " nan,\n",
       " 'blight',\n",
       " nan,\n",
       " 'blight',\n",
       " 'blight',\n",
       " 'blight',\n",
       " 'blight',\n",
       " 'tragedy',\n",
       " nan,\n",
       " 'blight',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'blizzard',\n",
       " nan,\n",
       " 'blizzard',\n",
       " 'blizzard',\n",
       " 'blizzard',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'blizzard',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'blizzard',\n",
       " 'blizzard',\n",
       " 'blizzard',\n",
       " nan,\n",
       " 'blizzard',\n",
       " 'blizzard',\n",
       " nan,\n",
       " 'blizzard',\n",
       " nan,\n",
       " nan,\n",
       " 'blizzard',\n",
       " nan,\n",
       " 'blizzard',\n",
       " 'blizzard',\n",
       " 'blizzard',\n",
       " 'blizzard',\n",
       " nan,\n",
       " 'blizzard',\n",
       " 'blizzard',\n",
       " nan,\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " nan,\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " nan,\n",
       " 'blood',\n",
       " 'blood',\n",
       " nan,\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'blood',\n",
       " 'lightning',\n",
       " 'blood',\n",
       " nan,\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " nan,\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " nan,\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " nan,\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " 'bloody',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'flames',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'bloody',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 'dead',\n",
       " 'wreck',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " ...]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                        0\n",
       "keyword                  87\n",
       "target                 3263\n",
       "text_len                  0\n",
       "capital_letters           0\n",
       "punctuations              0\n",
       "clean_text                0\n",
       "remove_punctuations       0\n",
       "all_keywords           1663\n",
       "dtype: int64"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_feature['all_keywords'] = all_keywords\n",
    "df_all_feature.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                        0\n",
       "keyword                  87\n",
       "target                 3263\n",
       "text_len                  0\n",
       "capital_letters           0\n",
       "punctuations              0\n",
       "clean_text                0\n",
       "remove_punctuations       0\n",
       "all_keywords           1663\n",
       "temp_key                 87\n",
       "dtype: int64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_feature['temp_key'] = df_all_feature['keyword']\n",
    "df_all_feature.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'temp_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-243-025536f83d47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_all_feature\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'temp_key'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_all_feature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfill_missing_keyword_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'remove_punctuations'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'temp_key'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'temp_key'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_all_feature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i318517\\desktop\\python\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   6876\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6877\u001b[0m         )\n\u001b[1;32m-> 6878\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6880\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DataFrame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i318517\\desktop\\python\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i318517\\desktop\\python\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m                 result = libreduction.compute_reduction(\n\u001b[1;32m--> 296\u001b[1;33m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 )\n\u001b[0;32m    298\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.compute_reduction\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-243-025536f83d47>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_all_feature\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'temp_key'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_all_feature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfill_missing_keyword_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'remove_punctuations'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'temp_key'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'temp_key'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_all_feature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i318517\\desktop\\python\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i318517\\desktop\\python\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   4402\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4403\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4404\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4405\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4406\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._maybe_get_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'temp_key'"
     ]
    }
   ],
   "source": [
    "df_all_feature['temp_key'] = df_all_feature.apply(lambda row: fill_missing_keyword_values(row['remove_punctuations']) if np.isnan(row['temp_key']) else row['temp_key'])\n",
    "df_all_feature.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    earthquake\n",
       "1          fire\n",
       "2    evacuation\n",
       "3    evacuation\n",
       "4         smoke\n",
       "Name: all_keywords, dtype: object"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_feature['all_keywords'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    NaN\n",
       "1    NaN\n",
       "2    NaN\n",
       "3    NaN\n",
       "4    NaN\n",
       "Name: temp_key, dtype: object"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_feature['temp_key'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>target</th>\n",
       "      <th>text_len</th>\n",
       "      <th>capital_letters</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>133</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword  target  text_len  capital_letters  punctuations  \\\n",
       "0   1     NaN     1.0        69               10             1   \n",
       "1   4     NaN     1.0        38                5             1   \n",
       "2   5     NaN     1.0       133                2             3   \n",
       "3   6     NaN     1.0        65                1             2   \n",
       "4   7     NaN     1.0        88                3             2   \n",
       "\n",
       "                                          clean_text  \n",
       "0          deed reason earthquak may allah forgiv us  \n",
       "1               forest fire near la rong sask canada  \n",
       "2  resid ask shelter place notifi offic evacu she...  \n",
       "3  13000 peopl receiv wildfir evacu order california  \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...  "
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = df_all_feature.drop(['temp_key','all_keywords','remove_punctuations'],axis=1)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>target</th>\n",
       "      <th>text_len</th>\n",
       "      <th>capital_letters</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.956250</td>\n",
       "      <td>-0.003884</td>\n",
       "      <td>-1.284438</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.872349</td>\n",
       "      <td>-0.469166</td>\n",
       "      <td>-1.284438</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.935051</td>\n",
       "      <td>-0.748334</td>\n",
       "      <td>-0.847022</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.074457</td>\n",
       "      <td>-0.841391</td>\n",
       "      <td>-1.065730</td>\n",
       "      <td>13000 peopl receiv wildfir evacu order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.394770</td>\n",
       "      <td>-0.655278</td>\n",
       "      <td>-1.065730</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword  target  text_len  capital_letters  punctuations  \\\n",
       "0   1     NaN     1.0 -0.956250        -0.003884     -1.284438   \n",
       "1   4     NaN     1.0 -1.872349        -0.469166     -1.284438   \n",
       "2   5     NaN     1.0  0.935051        -0.748334     -0.847022   \n",
       "3   6     NaN     1.0 -1.074457        -0.841391     -1.065730   \n",
       "4   7     NaN     1.0 -0.394770        -0.655278     -1.065730   \n",
       "\n",
       "                                          clean_text  \n",
       "0          deed reason earthquak may allah forgiv us  \n",
       "1               forest fire near la rong sask canada  \n",
       "2  resid ask shelter place notifi offic evacu she...  \n",
       "3  13000 peopl receiv wildfir evacu order california  \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...  "
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scale data using standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler()\n",
    "df_new [['text_len','capital_letters','punctuations']]= scale.fit_transform(df_new[['text_len','capital_letters','punctuations']])\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text_len</th>\n",
       "      <th>capital_letters</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>27464</th>\n",
       "      <th>27465</th>\n",
       "      <th>27466</th>\n",
       "      <th>27467</th>\n",
       "      <th>27468</th>\n",
       "      <th>27469</th>\n",
       "      <th>27470</th>\n",
       "      <th>27471</th>\n",
       "      <th>27472</th>\n",
       "      <th>27473</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.956250</td>\n",
       "      <td>-0.003884</td>\n",
       "      <td>-1.284438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.872349</td>\n",
       "      <td>-0.469166</td>\n",
       "      <td>-1.284438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.935051</td>\n",
       "      <td>-0.748334</td>\n",
       "      <td>-0.847022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.074457</td>\n",
       "      <td>-0.841391</td>\n",
       "      <td>-1.065730</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.394770</td>\n",
       "      <td>-0.655278</td>\n",
       "      <td>-1.065730</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  27478 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  text_len  capital_letters  punctuations  0  1  2  3  4  5  ...  \\\n",
       "0     1.0 -0.956250        -0.003884     -1.284438  0  0  0  0  0  0  ...   \n",
       "1     1.0 -1.872349        -0.469166     -1.284438  0  0  0  0  0  0  ...   \n",
       "2     1.0  0.935051        -0.748334     -0.847022  0  0  0  0  0  0  ...   \n",
       "3     1.0 -1.074457        -0.841391     -1.065730  0  0  0  0  0  0  ...   \n",
       "4     1.0 -0.394770        -0.655278     -1.065730  0  0  0  0  0  0  ...   \n",
       "\n",
       "   27464  27465  27466  27467  27468  27469  27470  27471  27472  27473  \n",
       "0      0      0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 27478 columns]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encoding data using Count Venctorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_count = count_vect.fit_transform(df_new['clean_text'])\n",
    "df_new.reset_index(drop=True, inplace=True)\n",
    "X_count_feat = pd.concat([df_new['target'],df_new['text_len'],df_new['capital_letters'],df_new['punctuations'],\n",
    "                           pd.DataFrame(X_count.toarray())], axis=1)\n",
    "#X_count_feat = pd.DataFrame(X_count.toarray())\n",
    "X_count_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 27478)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new = X_count_feat[X_count_feat.target.notnull()]\n",
    "train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 27478)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new = X_count_feat[X_count_feat.target.isnull()]\n",
    "test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_new.drop(['target'],axis=1)\n",
    "y_train = train_new['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 27477)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = test_new.drop(['target'],axis=1)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61785949 0.54103743 0.56992777 0.57095926 0.55978975]\n",
      "0.5719147405140452\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "cv = cross_val_score(gnb,x_train,y_train,cv=5)\n",
    "print(cv)\n",
    "print(cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70321733 0.63624425 0.6690742  0.68199737 0.74770039]\n",
      "0.6876467101465656\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "lr = LogisticRegression(max_iter = 2000)\n",
    "cv = cross_val_score(lr,x_train,y_train,cv=5)\n",
    "print(cv)\n",
    "print(cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6651346  0.56795798 0.56861458 0.62943495 0.69448095]\n",
      "0.6251246114117047\n"
     ]
    }
   ],
   "source": [
    "#decision tree classifier\n",
    "dt = tree.DecisionTreeClassifier(random_state = 1)\n",
    "cv = cross_val_score(dt,x_train,y_train,cv=5)\n",
    "print(cv)\n",
    "print(cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.60472751 0.62967827 0.59553513 0.63731932 0.67871222]\n",
      "0.6291944887114184\n"
     ]
    }
   ],
   "source": [
    "#K nearest neighbour\n",
    "knn = KNeighborsClassifier()\n",
    "cv = cross_val_score(knn,x_train,y_train,cv=5)\n",
    "print(cv)\n",
    "print(cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69862114 0.62836507 0.68745896 0.6478318  0.75886991]\n",
      "0.6842293764554535\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "rf = RandomForestClassifier(random_state = 1)\n",
    "cv = cross_val_score(rf,x_train,y_train,cv=5)\n",
    "print(cv)\n",
    "print(cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69271175 0.59816152 0.60078792 0.62943495 0.72207622]\n",
      "0.6486344729047294\n"
     ]
    }
   ],
   "source": [
    "#xgboost\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(random_state =1)\n",
    "cv = cross_val_score(xgb,x_train,y_train,cv=5)\n",
    "print(cv)\n",
    "print(cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(probability = True)\n",
    "cv = cross_val_score(svc,x_train,y_train,cv=5)\n",
    "print(cv)\n",
    "print(cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=2000)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRclassifier = LogisticRegression(max_iter = 2000)\n",
    "LRclassifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalised_model.pkl'\n",
    "pickle.dump(LRclassifier,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=LRclassifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 0.])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR submission without HPO\n",
    "pred = pd.DataFrame(y_pred)\n",
    "sub_fr = pd.read_csv('sample_submission.csv')\n",
    "datasets = pd.concat([sub_fr['id'],pred],axis=1)\n",
    "datasets.columns = ['id','target']\n",
    "datasets.to_csv('sample_submission.csv',index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
